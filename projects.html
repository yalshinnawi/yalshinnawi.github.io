<!DOCTYPE html>

<html lang="en">
<head>
    <!--- Basic Page Needs -->
    <meta charset="utf-8">
    <title>Yousef Al-Shinnawi</title>

    <!---Mobile Specific Metas -->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!---CSS -->
    <link rel="stylesheet" href="css/default.css">
    <link rel="stylesheet" href="css/layoutProjects.css">
    <link rel="stylesheet" href="css/media-queries.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    
    <!---Script -->
    <script src="js/modernizr.js"></script>
    
    <!-- jQuery library -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    
    <!-- Latest compiled JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>

<body>
    <!--Header -->
    <header id="project-header">

        <nav id="nav-wrap">

            <a class="mobile-btn" href="#nav-wrap" title="Show navigation">Show navigation</a>
            <a class="mobile-btn" href="#" title="Hide navigation">Hide navigation</a>

            <ul id="nav" class="nav">
                <li class="current"><a href="index.html">Home</a></li>
            </ul> <!-- end #nav -->

            
        </nav> <!-- end #nav-wrap -->

    </header>
    
    <section id="projects">
        <div class="container-fluid">
            <div class="jumbotron text-center bg-primary">
                <h1>Projects</h1>
            </div>
        </div>

        <div class="panel panel-default center-block" style="width: 50%">
            <div class="panel-heading text-center">
                CPU Design
            </div>
            <div class="panel-body">
                <div class="row">
                    <div class="col-lg-6">
                        <p>
                            The goal of this project was to design and implement a 3-stage pipelined Central Processing Unit (CPU) using the
                            RISC-V
                            ISA programmed in Verilog to run on a Zynq 7000-series FPGA. In addition to having a processor that could run
                            the base
                            RV32 instruction set, there had to also be separate modules in the datapath for handling input/output (IO) to
                            transmit
                            and receive data. An on-chip Universial Asynchronous Reciever-Transmitter (UART) using the technique of Memory
                            Mapped IO
                            (MMIO) was to be implemented to send and receive bytes using RISC-V load and store instructions. This interface
                            would
                            then allow the CPU to store the user’s instructions into BIOS memory which could then be read on the following
                            cycle
                            instead of reading from Instruction Memory (IMEM).
                        </p>
                    </div>
                    <div class="col-lg-6" style="text-align: center;">
                        <img src="images/EECS151L Final Project High-level Overview of System.png" alt="">
                        <figcaption>
                            CPU High Level Overview
                        </figcaption>
                    </div>
                </div>
            </div>
            <button data-toggle="collapse" data-target="#cpuDesign">
                Show More
            </button>
            <div id="cpuDesign" class="panel collapse">
                <div class="row">
                    <div class="col-lg-12">
                        <p>
                            The processor was divided into 3 sections, the Instruction Fetch Stage (IF Stage), Instruction Decode and Execute Stage
                            (IDEX Stage), and Memory Write Back Stage (MEMWB Stage). In the IF Stage, new instructions are loaded from either the
                            IMEM or BIOS which are then decoded and passed through the Register File (RegFile) to load data for the IDEX Stage. The
                            IDEX Stage then passes this data through combinational submodules such as the Arithmetic Logic Unit (ALU) or Branch
                            Comparator to compute the logic of the current instruction. Output from this stage is saved to the IMEM and Data Memory
                            (DMEM) which is then passed through to the MEMWB Stage. Following this, the MEMWB Stage handles what data is to be
                            written back to the RegFile for the current instruction.
                        </p>
                    </div>
                </div>
                <div class="row" style="text-align: center;">
                    <div class="col-lg-4">
                        <img src="images/EECS151DataPath-IF Stage.jpg" alt="">
                    </div>
                    <div class="col-lg-4">
                        <img src="images/EECS151DataPath-IDEX Stage.jpg" alt="">
                    </div>
                    
                    <div class="col-lg-4">
                        <img src="images/EECS151DataPath-MEMWB Stage.jpg" alt="">
                    </div>
                </div>
                <div class="row" style="text-align: center;"> 
                    <div class="col-lg-4">
                        IF Stage
                    </div>
                    <div class="col-lg-4">
                        IDEX Stage
                    </div>
                    <div class="col-lg-4">
                        MEMWB Stage
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-12">
                        <p>
                            <span>
                                In terms of the original goal, the CPU checked all the marks. The entire datapath was implemented with a 3-stage
                                pipeline that could send and receive data through the UART MMIO interface. Through this interface, the CPU was able to
                                interact with on-board buttons, switches, LEDS, and the audio output with the Subtractive Synthesizer.
                            </span><br>

                            <span>
                                By the final checkpoint, the processor was able to reach a clock speed of ~70MHz. The max path delay turned out to be in
                                the IDEX Stage of the 3-stage pipeline. Specifically the delay was between the initial instruction input to the IDEX
                                Stage to the Synthesizer block at the end of this stage. It is speculated that this delay occurs because of the need for
                                signed multiplication and buffer that exists in the PWM block for delivering audio to the on-board audio port.
                                Multiplication is a very intensive process and the current method used in this design is not optimal.
                            </span><br>

                            <span>
                                Additionally, it is important to mention that before implementing the NCO, PWM and Synthesizer, the critical path
                                instead existed between the MEMWB stage and IDEX stage during forwarding. In the previous design we had reached a final
                                clock speed of ~76MHz. It is believed that this critical path existed because of the need for the RegFile return address
                                and write enable from 2 cycles ago.
                            </span><br>

                            <span>
                                For both designs, the CPI of the mmult program was ~1.18 most likely due to the design’s scheme of handling jumps and
                                branch instructions by simply adding a NOP to the next cycle. The minimum clock period for the final design was 0.214s
                                for the mmult program, the number of Slice LUTs used was 2,283, and the number of Slice Registers used was 905.
                            </span>
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <div class="panel panel-default center-block" style="width: 50%">
            <div class="panel-heading text-center">
                Yoga Pose Correction
            </div>
            <div class="panel-body">
                <div class="row">
                    <div class="col-lg-6" style="text-align: center;">
                        <img src="images/RoboticsCoverPic.png" alt="">
                        <figcaption>
                            Baxter Robot and the HumanBot Model
                        </figcaption>
                    </div>
                    <div class="col-lg-6">
                        <p>
                            <span>
                                This project was inspired by real world challenges that have been caused by the COVID-19 pandemic. Because of social
                                distancing precautions people are unable to participate in typical yoga classes. Yoga instructors often have to put
                                their hands on their students in order to help them with their poses.
                            </span><br>
                            
                            <span>
                                This project attempts to use the popular Rethink Robotics robot Baxter as a makeshift yoga instructor. The goal of this
                                project is to correct a person's pose. Baxter reaches out and nudges the person's arms from their original position into
                                the desired one, attempting to mimic what an instructor would do.
                            </span><br>

                            <span>
                                The overall goal of the project was to correct a person's body position using Baxter. More specifically, in our initial
                                project proposal we planned to move someone’s arms into the YMCA poses, but after we began to work through the project
                                we had to tone this down quite a bit. As a result, our goal turned into just getting Baxter to move someone’s arms into
                                a T-Pose position as shown in the figure.
                            </span><br>

                            <span>
                                Because of the COVID-19 pandemic all of this work was completed using Gazebo simulation software to model Baxter and the
                                human participant.
                            </span>
                        </p>
                    </div>
                </div>
            </div>
            <button data-toggle="collapse" data-target="#yogaPose">
                Show More
            </button>
            <div id="yogaPose" class="panel collapse">
                <div class="row">
                    <div class="col-lg-12">
                        <p>
                            In order to accomplish our project goals this really required two main components: the mannequin's pose and a plan to
                            move the mannequin’s arms. To acquire the mannequin's pose, we used Baxter's head camera and arUco markers. We chose to
                            use arUco tags because they make it possible to get the three dimensional position of the tag using only one camera.
                            They are also fairly well documented and there are libraries available to detect arUco markers. While arUco tags may be
                            a bit cumbersome for a user in a real life scenario they offer a simple and well tested solution for defining the
                            location of objects in 3D space using just one camera. In future iterations it may make sense to use a RealSense camera
                            or IMUs for pose estimation.
                            Each aspect of the simulation/project design is further discussed below in the Project Details section.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-6">
                        <h2>
                            ArUco Tags
                        </h2>
                        <p>
                            There are several ways to get the pose of a person. Kinect, IMUs, or other sensors might work. We decided to use arUco
                            tags from the OpenCV library. These are pre-defined tags that allow a single camera to find the frame of the tag
                            relative to the camera in 3D space. Luckily enough there is already a ROS package for detecting arUco tags. In this
                            design we ended up using the aruco_detect package. By subscribing to the fiducial_transforms topic published by the
                            aruco_detect package, we were able to get the frame of the arUco tag on the mannequin's arm in the head_camera frame.
                            This pose was then transformed to the base frame using a tf_buffer.
                        </p>
                    </div>

                    <div class="col-lg-6">
                        <img src="images/roboticsArucoTagPic.webp" alt="">
                        <figcaption>
                            ArUco Tag Detection
                        </figcaption>
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-6">
                        <img src="images/MannequinPic.jpg" alt="">
                        <figcaption>
                            Mannequin Spawn
                        </figcaption>
                    </div>
                
                    <div class="col-lg-6">
                        <h2>
                            HumanBot - Spawning the Mannequin
                        </h2>
                        <p>
                            HumanBot is the name that we gave the mannequin. HumanBot is defined by a urdf.xacro file. This file defines his links,
                            joints, and physics.
                            
                            HumanBot’s arms defined to move up and down so that they can be relocated by Baxter. Additionally, as can be seen in the
                            picture to the right, there are two arUco tags on the mannequin's arms. These are modeled as fixed joints onto the arm
                            and allow the head_camera to detect the arm.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-6">
                        <h2>
                            Motion Planning with Baxter
                        </h2>
                        <p>
                            Finally, with this new pose from the aruco_detect package we were able to use the MoveIt package to handle the path
                            planning and collision detection. The ultimate reason for using MoveIt instead of simply using an Inverse Kinematics
                            solver was because initially we had planned to add resistance to HumanBot's arm and the path planner would allow for
                            a
                            sense of control as the robot continues to move the arms.
                        </p>
                    </div>
                
                    <div class="col-lg-6">
                        <img src="images/roboticsMotionPlanning.webp" alt="">
                        <figcaption>
                            Baxter and HumanBot
                        </figcaption>
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-12">
                        <div style="text-align: center;">
                            <video id="roboticsVideo" width="900" height="900">
                                <source src="videos/RoboticsDemo.mp4" type="video/mp4">
                                Your browser does not support HTML video.
                            </video>
                            <button onclick="playPause()">
                                Play/Pause
                            </button>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="panel panel-default center-block" style="width: 50%">
            <div class="row">
                <div class="col-lg-4 text-center">
                    <h2>
                        Gitlet
                    </h2>
                </div>
                <div class="col-lg-4 text-center">
                    Designed a version control system mimicking the basic features of Git.
                </div>
                <div class="col-lg-4">
                    
                </div>
            </div>
        </div>

        <div class="panel panel-default center-block" style="width: 50%">
            <div class="row">
                <div class="col-lg-4 text-center">
                    <h2>
                        Robot Crawler
                    </h2>
                </div>
                <div class="col-lg-4 text-center">
                    Implemented different versions of value iteration and Q-learning based on the Bellman Ford Algorithm to
                    train a robot crawler and Pacman.
                </div>
                <div class="col-lg-4">

                </div>
            </div>
        </div>
    </section>

    <!-- footer
        ================================================== -->
    <!-- <footer>
    
        <div class="row">
    
            <div class="twelve columns">
    
                <ul class="social-links">
                    <li><a href="https://www.linkedin.com/in/yousef-alshinnawi/" target="_blank"><i
                                class="fa fa-linkedin"></i></a></li>
                    <li><a href="https://github.com/yalshinnawi" target="_blank"> <i class="fa fa-github"></i></a></li>
                </ul>
            </div>
    
            <div id="go-top"><a class="smoothscroll" title="Back to Top" href="#home"><i class="icon-up-open"></i></a></div>
    
        </div>
    
    </footer>  -->
    <!-- Footer End-->
    <script>
        var myVideo = document.getElementById("roboticsVideo")

        function playPause() {
            if (myVideo.paused)
                myVideo.play();
            else
                myVideo.pause();
        }
    </script>
</body>
</html>